% !Mode:: "TeX:UTF-8"

\chapter{语音识别的基本原理}\label{intro_hmm}

传统语音识别系统的基本原理是隐马尔可夫模型HMM\ucite{rabiner1989tutorial,huang2001spoken,bishop2006pattern, rabiner1993fundamentals}，
基础系统为HMM-GMM系统。

隐马尔科夫模型（HMM）是一种统计模型，已广泛应用于语音识别、语音合成、文字识别等序列任务中。
二十世纪七十年代Baum等人建立了HMM的理论基础，
随后，Baker和Jelinek等人将其成功应用在语音识别任务中。
混合高斯模型GMM使用多个高斯概率密度函数描述概率分布，可以对连续概率密度分布进行建模。
HMM-GMM系统中，使用HMM描述语音发音的状态变化过程，使用GMM表述不同状态的概率密度分布，
两者结合实现大词汇量连续语音识别。


本章介绍基于HMM-GMM的语音识别的基本原理。

%\newtheorem{definition}{\textbf{定义}}[section]
%\newtheorem{algorithm}{\hspace{2em}\textbf{算法}}[section]
%
\section{隐马尔科夫模型的概念}

    \begin{definition}[隐马尔科夫模型]
    \song 隐马尔科夫模型是关于时间序列的概率模型，它描述由隐马尔科夫链随机生成未知的状态序列，
    然后由隐状态生成对应观测从而产生观测序列的过程。
    所以，隐马尔科夫模型中包含两个序列，一是状态序列，二是观测序列，每个状态对应一个观测。
    \end{definition}

    隐马尔科夫模型的基本要素为初始概率、状态转移概率和观测概率，三者共同确定隐马尔科夫模型，其形式定义如下：

    记$Q$表示状态集合，即所有可能的状态，
    $V$表示观测集合，即所有可能的观测。
    \[Q = \{ {q_1},{q_2},\cdots,{q_N}\} , V = \{ {v_1},{v_2},\cdots,{v_M}\} \]
    其中，$N$表示状态数，$M$表示观测数。

    $I$表示状态序列，$T$表示其长度，$O$表示该状态序列对应的观测序列。
    \[I = ({i_1},{i_2},\cdots,{i_T}), O = ({o_1},{o_2},\cdots,{o_T})\]

    $A$表示状态转移矩阵：
    \begin{equation}
    A = {\left[ {{a_{ij}}} \right]_{N \times N}}
    \end{equation}
    其中，
    \begin{equation}
    {a_{ij}} = {\kern 1pt} P({i_{t + 1}} = {q_j}|{i_t} = {q_i}),{\kern 1pt} {\kern 1pt} i = 1,2, \cdots ,N;{\kern 1pt} j = 1,2, \cdots ,N
    \end{equation}
    表示在$t$时刻到$t+1$，由状态$q_i$跳转到状态$q_j$的概率。

    $B$表示观测概率矩阵：
    \begin{equation}
    B = {\left[ {{b_{j}(k)}} \right]_{N \times M}}
    \end{equation}
    其中，
    \begin{equation}
    {b_{j}(k)} = {\kern 1pt} P({o_t} = {v_k}|{i_t} = {q_j}),{\kern 1pt} {\kern 1pt} k = 1,2, \cdots ,N;{\kern 1pt} j = 1,2, \cdots ,N
    \end{equation}
    表示在$t$时刻状态$q_j$的生成观测$v_k$的概率。

    $\pi$是初始状态概率：
    \begin{equation}
    \pi  = ({\pi _i})
    \end{equation}
    其中，
    \begin{equation}
    {\pi _i} = P({i_1} = {q_i}),i = 1,2, \cdots N
    \end{equation}
    表示在初始时刻，即$t=1$时处于状态$q_i$的概率。

    综上，可以使用三元组$\lambda $表示隐马尔科夫模型，即
    \begin{equation}
    \lambda  = \left( {A,B,\pi } \right)
    \end{equation}
    ${A,B,\pi }$称为隐马尔科夫模型的三要素。
    其中，初始状态概率向量$\pi$和状态转移概率矩阵$A$确定了状态序列；
    观测概率矩阵$B$确定了在某个状态上生成某种观测的概率，
    两者综合作用产生了观测序列。

    通过定义可知，隐马尔科夫模型有两个基本的假设：
    \begin{enumerate}
    	\item 齐次马尔科夫性假设，即在任意时刻$t$时刻状态只依赖于前一时刻的状态，与之前的时刻无关。
            \begin{equation}
            P({i_t}|{t_{t - 1}},{o_{t - 1,}} \cdots {i_1},{o_1}) = P({i_t}|{i_{t - 1}}),t = 1,2, \cdots T
            \end{equation}
        \item 观测独立性假设，即在$t$时刻的观测仅与其所处的状态相关，而与其他时刻的观测和状态无关。
            \begin{equation}
            P({o_t}|{t_T},{o_{T,}} \cdots ,{i_t},{o_t}, \cdots ,{i_1},{o_1}) = P({o_t}|{i_t})
            \end{equation}
    \end{enumerate}

\section{HMM的三个基本问题}
    隐马尔科夫模型有3个基本问题：
    \begin{enumerate}
        \item 概率问题。给定模型$\lambda  = \left( {A,B,\pi } \right)$和观测序列$O = ({o_1},{o_2},\cdots,{o_T})$，计算在观测序列$O$发生的概率$P(O|\lambda )$。
        \item 学习问题。
            已知观测序列$O = ({o_1},{o_2},\cdots,{o_T})$，
            估计隐马尔科夫模型$\lambda  = \left( {A,B,\pi } \right)$的参数，
            使得在该参数下观测到$O$的概率$P(O|\lambda )$最大。
        \item 预测问题。
            已知模型$\lambda  = \left( {A,B,\pi } \right)$和观测序列$O = ({o_1},{o_2},\cdots,{o_T})$，
            求对应的最有可能的状态序列$I = ({i_1},{i_2},\cdots,{i_T})$。
    \end{enumerate}

    \subsection{概率问题}\label{section:probability}
    本节介绍计算观测序列概率$P(O|\lambda )$的前向算法与后向算法，为方便后面计算学习问题，结合前向后向算法，本节第三小节定相关数学符号。
        \subsubsection{前向算法}
        \begin{definition}[前向算法]
        \song 给定隐马尔科夫模型$\lambda$，
        定义在时刻$t$处于状态$q_i$，且观测到$({o_1},{o_2},\cdots,{o_t})$的概率为前向概率，记作
            \begin{equation}
            {\alpha _t}(i) = P({o_1},{o_2}, \cdots {o_t},{i_t} = {q_i}|\lambda )
            \end{equation}
        \end{definition}

        通过递推，可以求得前向概率${\alpha _t}(i)$，从而进一步得到观测序列概率$P(O|\lambda )$，算法描述如下：
        \begin{enumerate}
            \item 初值
                \begin{equation}
                    {\alpha _1}(i) = {\pi _i}{b_i}({o_1}),i = 1,2, \cdots N
                \end{equation}
            \item 前向递推~~~~对$t = 1,2, \cdots ,T - 1$，如图\ref{fig:forback}，
                \begin{equation}
                    {\alpha _{t + 1}}(i) = \left[ {\sum\limits_{j = 1}^N {{\alpha _t}(j){a_{ji}}} } \right]{b_i}({o_{t + 1}}),i = 1,2, \cdots ,N
                \end{equation}
            \item 终止
                \begin{equation}
                    P(O|\lambda ) = \sum\limits_{i = 1}^N {{\alpha _T}(i)}
                \end{equation}
        \end{enumerate}

        \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.9\textwidth]{figures/chapter2/forback-crop}
        \caption{前向后向概率计算示意图}
        \label{fig:forback}
        \end{figure}

        \subsubsection{后向算法}
        \begin{definition}[后向算法]
        \song 给定隐马尔科夫模型$\lambda$，
        定义在时刻$t$处于状态$q_i$，从$t+1$到$T$的部分观测序列为$({o_{t+1}},{o_{t+2}},\cdots,{o_T})$的概率为后向概率，记作
             \begin{equation}
                 {\beta _t}(i) = P({o_{t + 1}},{o_{t + 2}}, \cdots ,{o_T}|{i_t} = {q_i},\lambda )
             \end{equation}
        \end{definition}

        则可递推地求得后向概率${\beta _t}(i)$及观测序列概率$P(O|\lambda )$，算法描述如下：
        \begin{enumerate}
            \item 初值
                \begin{equation}
                    {\beta _T}(i) = 1,i = 1,2, \cdots ,N
                \end{equation}
            \item 后向递推,对于$t = T - 1,T - 2, \cdots ,1$，如图\ref{fig:forback}，
                 \begin{equation}
                    {\beta _t}(i) = \sum\limits_{j = 1}^N {{a_{ij}}{b_j}({o_{t + 1}}){\beta _{t + 1}}(j),i = 1,2, \cdots N}
                 \end{equation}
            \item 终止
                \begin{equation}
                    P(O|\lambda ) = \sum\limits_{i = 1}^N {{\pi _i}{b_i}({o_1}){\beta _1}\left( i \right)}
                \end{equation}
        \end{enumerate}

        \subsubsection{数学符号定义}
        利用前向概率和后向概率，我们引出以下概念
        \begin{enumerate}
            \item 观测序列概率$P(O|\lambda )$的统一形式：
                \begin{equation}
                    P(O|\lambda ) = \sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N {{\alpha _t}(i)} {a_{ij}}{b_j}({o_{t + 1}}){\beta _{t + 1}}\left( j \right),} t = 1,2, \cdots T - 1
                \end{equation}
            \item 给定模型$\lambda $和观测$O$，在时刻$t$处于状态$q_i$的概率，记
                \begin{equation}
                    {\gamma _t}(i) = P({i_t} = {q_i}|O,\lambda )
                \end{equation}
                可以通过前向后向概率计算。事实上，
                \[{\gamma _t}(i) = P({i_t} = {q_i}|O,\lambda ) = \frac{{P({i_t} = {q_i},O|\lambda )}}{{P(O|\lambda )}}\]
                由前向后向概率定义可知：
                \[{\alpha _t}(i){\beta _t}(i) = P({i_t} = {q_i},O|\lambda )\]
                如图\ref{fig:gamma}，于是有：
                \begin{equation}\label{equation:gamma}
                {\gamma _t}(i) = \frac{{{\alpha _t}(i){\beta _t}(i)}}{{P(O|\lambda )}} = \frac{{{\alpha _t}(i){\beta _t}(i)}}{{\sum\limits_{j = 1}^N {{\alpha _t}(j){\beta _t}(j)} }}
                \end{equation}

                \begin{figure}[htbp]
                \centering
                \includegraphics[width=0.5\textwidth]{figures/chapter2/gamma-crop}
                \caption{${\gamma _t}(i)$计算示意图}
                \label{fig:gamma}
                \end{figure}

            \item 定模型$\lambda $和观测$O$，在时刻$t$处于状态$q_i$且在$t+1$时刻处于状态$q_j$的概率记作
                \begin{equation}
                    {\xi _t}(i,j) = P({i_t} = {q_i},{i_{t + 1}} = {q_j}|O,\lambda )
                \end{equation}
                通过前向后向概率计算得：
                \[{\xi _t}(i,j) = \frac{{P({i_t} = {q_i},{i_{t + 1}} = {q_j},O|\lambda )}}{{P(O|\lambda )}} = \frac{{P({i_t} = {q_i},{i_{t + 1}} = {q_j},O|\lambda )}}{{\sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N {P({i_t} = {q_i},{i_{t + 1}} = {q_j},O|\lambda )} } }}\]
                而
                \[P({i_t} = {q_i},{i_{t + 1}} = {q_j},O|\lambda ) = {\alpha _t}(i){a_{ij}}{b_j}({o_{t + 1}}){\beta _{t + 1}}(j)\]
                所以
                \begin{equation}\label{equation:xi}
                {\xi _t}(i,j) = \frac{{{\alpha _t}(i){a_{ij}}{b_j}({o_{t + 1}}){\beta _{t + 1}}(j)}}{{\sum\limits_{i = 1}^N {\sum\limits_{j = 1}^N {{\alpha _t}(i){a_{ij}}{b_j}({o_{t + 1}}){\beta _{t + 1}}(j)} } }}
                \end{equation}
        \end{enumerate}
    \subsection{学习问题}\label{section:learn}
        假设训练数据只包含$S$个长度为T的观测序列$\{ {O_1},{O_2}, \cdots ,{O_s}\} $，状态序列未知。
        学习问题即是求隐马尔科夫模型$\lambda  = \left( {A,B,\pi } \right)$的参数，使观测到$O$的概率最大。
        未知的状态序列即为模型中的隐变量。
        \begin{equation}
        P(O|\lambda ) = \sum\limits_I {P(O|I,\lambda )P(I|\lambda )}
        \end{equation}
        它的参数学习可以通过最大似然估计和EM算法实现。

        EM\ucite{dempster1977maximum}算法是一种迭代算法，1977年由Dempster等人总结提出，
        用于含有隐变量的概率模型参数的极大似然估计，或极大后验估计。
        EM算法每次迭代由两步组成：
        E步，求期望（Expectation）；
        M步，求极大（Maximization）。
        所以这一算法称为期望极大算法（Expectation Maximization Algorithm），简称EM算法。
        Baum-Welch算法是EM算法在隐马尔科夫模型学习中的具体实现，其算法步骤如下：
        \begin{enumerate}
            \item 初始化，
                对于$n=0$,选取$a_{ij}^{(0)}$，${b_j}{(k)^{(0)}}$，$\pi _i^{(0)}$，得到模型${\lambda ^{(0)}} = \left( {{A^{(0)}},{B^{(0)}},{\pi ^{(0)}}} \right)$。
            \item 递推，对于$n = 1,2, \cdots $，
                    \[a_{ij}^{(n + 1)} = \frac{{\sum\limits_{t = 1}^{T - 1} {{\xi _t}(i,j)} }}{{\sum\limits_{t = 1}^{T - 1} {{\gamma _t}(i)} }}\]
                    \[{b_j}{(k)^{(n + 1)}} = \frac{{\sum\limits_{t = 1,{o_t} = {v_k}}^T {{\gamma _t}(j)} }}{{\sum\limits_{t = 1}^T {{\gamma _t}(j)} }}\]
                    \[\pi _i^{(n + 1)} = {\gamma _1}(i)\]
                    右端各值按观测$O = \{ {o_1},{o_2}, \cdots ,{o_T}\}$和模型${\lambda ^{(n)}} = \left( {{A^{(n)}},{B^{(n)}},{\pi ^{(n)}}} \right)$计算。
                    式中${{\gamma _t}(j)}$和${{\xi _t}(i,j)}$由式\ref{equation:gamma}和式\ref{equation:xi}给出。
            \item 终止。得到模型参数${\lambda ^{(n + 1)}} = \left( {{A^{(n + 1)}},{B^{(n + 1)}},{\pi ^{(n + 1)}}} \right)$。
        \end{enumerate}

    \subsection{预测问题}\label{section:predict}
    隐马尔科夫模型的预测问题可以由维特比算法（Viterbi算法）解决。
    维特比算法通过动态规划解隐马尔科夫模型预测问题，即用动态规划求最大概率路径。

    根据动态规划原理，最优路径具有这样的特性；如果最优路径在时刻$t$通过节点${i_t}^*$，
    那么这一路径从节点${i_t}^*$到终点${i_T}^*$的部分路径，
    对于从${i_t}^*$到${i_T}^*$的所有可能的部分路径来说，必须是最优的。
    因为如果不是这样，那么从${i_t}^*$到${i_T}^*$就有另一条更好的部分路径存在，
    如果把它和从${i_1}^*$到达${i_t}^*$的部分路径连接起来，就会形成比原来路径更优的路径，这是矛盾的。
    所以，从初始时刻递推计算在$t$时刻处于状态为$i$的所有可能路径的最大概率，
    最终得到$T$时刻所有可能路径的最大概率。
    时刻$t=T$的最大概率即为最大概率即为最有路径的概率${P^*}$，最优路径的终结点${i_T}^*$也同时得到。
    之后，为了找出最优路径的各个节点，从终结点${i_T}^*$开始，由后向前逐步
    求得节点${i_{T - 1}}^*, \cdots ,{i_1}^*$，得到最优路径${I^*} = ({i_1}^*,{i_2}^*, \cdots ,{i_{T - 1}}^*,{i_T}^*)$，这就是维特比算法。

    算法首先导入两个变量$\delta $和$\psi $。定义在时刻$t$状态为$i$的所有单个路径$({i_1},{i_2}, \cdots ,{i_t})$中概率最大值为
        \begin{equation}
            {\delta _t}(i) = \mathop {\max }\limits_{{i_1},{i_2}, \cdots ,{i_{t - 1}}} P({i_t} = i,{i_{t - 1}}, \cdots ,{i_1},{o_t}, \cdots ,{o_1}|\lambda ),i = 1,2,, \cdots ,N
        \end{equation}

    由定义可得变量$\delta $的递推公式：
        \begin{equation}
            \begin{array}{l}
            {\delta _{t + 1}}(i) = \mathop {\max }\limits_{{i_1},{i_2}, \cdots ,{i_t}} P({i_{t + 1}} = i,{i_t}, \cdots ,{i_1},{o_{t + 1}}, \cdots ,{o_1}|\lambda )\\
            \;\;\;\;\;\;\;\;\; = \mathop {\max }\limits_{1 \le j \le N} \left[ {{\delta _t}(j){a_{ji}}} \right]{b_i}({o_{t + 1}}),i = 1,2, \cdots ,N;t = 1,2, \cdots,T - 1
        \end{array}
        \end{equation}

    定义在时刻$t$状态为$i$的所有单个路径$({i_1},{i_2}, \cdots ,{i_t})$中概率最大的路径第$t-1$个节点为：
         \begin{equation}
            {\psi _t}(i) = arg\mathop {\max }\limits_{1 \le j \le N} [{\delta _{t - 1}}(j){a_{ji}}],i = 1,2, \cdots, N
        \end{equation}

    下面描述Viterbi算法的具体过程。

    算法输入为观测数据$O = \{ {o_1},{o_2}, \cdots ,{o_T}\}$和模型参数$\lambda  = \left( {A,B,\pi } \right)$，输出为最优路径${I^*} = ({i_1}^*,{i_2}^*, \cdots ,{i_{T - 1}}^*,{i_T}^*)$。算法步骤如下：
    \begin{enumerate}
        \item 初始化
            \[{\delta _1}(i) = {\pi _i}{b_i}({o_1}),i = 1,2, \cdots ,N\]
            \[{\psi _1}(i) = 0,i = 1,2, \cdots ,N\]
        \item 递推，对于$t = 1,2, \cdots,T$，
            \[{\delta _t}(i) = \mathop {\max }\limits_{1 \le j \le N} \left[ {{\delta _{t - 1}}(j){a_{ji}}} \right]{b_i}({o_t}),i = 1,2, \cdots ,N\]
            \[{\psi _t}(i) = arg\mathop {\max }\limits_{1 \le j \le N} [{\delta _{t - 1}}(j){a_{ji}}],i = 1,2, \cdots ,N\]
        \item 终止
             \[{P^*} = \mathop {\max }\limits_{1 \le i \le N} {\delta _T}(i)\]
             \[{i_T}^* = arg\mathop {\max }\limits_{1 \le i \le N} \left[ {{\delta _T}(i)} \right]\]
        \item 最优路径回溯。对于$t = T-1,T-2, \cdots,1$
            \[{i_t}^* = {\psi _{t + 1}}({i_{t + 1}}^*)\]
            从而求得最优路径${I^*} = ({i_1}^*,{i_2}^*, \cdots ,{i_{T - 1}}^*,{i_T}^*)$
    \end{enumerate}



\section{连续密度的隐马尔科夫模型}

    以上讨论中，我们都假定观测输出概率，也就是$B$参数的分布是离散的，本节介绍连续密度的隐马尔科夫模型（Continuous HMM CHMM)。
    在连续密度隐马尔科夫模型中，可以使用概率密度函数来表示观测输出概率。
    
    假设$\bm{X}$为观测向量，使用$b_{j}(\bm{X})$表示观测的输出概率。
    在传统HMM-GMM语音识别系统中，使用高斯概率密度函数对$b_{j}(\bm{X})$进行建模，
    语音识别中，观测特征一般为多维特征，
    所以选用多元高斯密度函数，
    如式\ref{equation:gm}所示。
    \begin{equation}\label{equation:gm}
       {b_j}(\bm{X}) = \frac{1}{{{{(2\pi )}^{p/2}}{|{\bm{\Sigma}} |_j^{ - 1}}}}\exp \left\{ { - \frac{1}{2}(\bm{X} - {{\bm{\mu}} _j}) {\bm{\Sigma}}_j^{ - 1}{{(\bm{X} - {{\bm{\mu}}_j})}^T}} \right\}
    \end{equation}

    另一方面，在实际应用中，单个高斯概率的建模能力有限，一般选择建模能力更强的混合高斯模型，如式\ref{equation:gmm}所示。
    \begin{equation}\label{equation:gmm}
       {b_j}(\bm{X}) = \sum\limits_{m = 1}^M {{\omega _{jm}}} \frac{1}{{{{(2\pi )}^{p/2}}|{\bm{\Sigma}} |_{jm}^{ - 1}}}\exp \left\{ { - \frac{1}{2}(\bm{X} - {{\bm{\mu}} _{jm}}){\bm{\Sigma}}_{jm}^{ - 1}{{(\bm{X} - {{\bm{\mu}} _{jm}})}^T}} \right\}
    \end{equation}

    其中$M$表示混合高斯数，即使用$M$个高斯叠加组合描述输出概率密度函数，${\omega_{jm}}$是混合系数，表示第$m$个高斯所占权重。混合高斯的引入，极大的增强了模型
    的描述能力。


\section{HMM在语音识别中的应用}
    结合HMM的三个基本问题和连续密度的HMM，本节介绍HMM在语音识别中的模型建立、训练过程和识别过程的基本应用。

    首先，要明确在语音识别任务中观测是什么、状态是什么。观测是人所发出的一段语音信号，人在说不同词时会产生不同的语音信号，不同的语音信号对应不同的信号特征（
    如幅度、频率、过零率等），我们对语音信号进行特征提取，即可作为HMM模型的观测向量。人在发出不同音素时对应不同的发音过程，状态即是对不同发音过程的描述。
    由于人的发音过程不可逆，即状态跳转不可逆，语音识别任务中使用特殊的HMM模型，称为左右模型，如图\ref{fig:left_right}所示，可以看出仅能跳转到
    自身或更高的状态。若以音素为基本单位建立HMM模型，则一般使用3状态的HMM描述，如图\ref{fig:asr_hmm}所示，其中1、5状态为扩展状态，1状态为起始状态，只能跳转到其它
    状态；5状态为结束状态。1、5状态均不产生任何观测输出，只是为了描述的方便，特别在HMM模型连接中非常有用。

    \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/chapter2/left_right-crop}
    \caption{左右模型HMM}
    \label{fig:left_right}
    \end{figure}

    \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/chapter2/asr_hmm-crop}
    \caption{语音识别中的5状态HMM}
    \label{fig:asr_hmm}
    \end{figure}

    模型的训练是一个不断迭代的过程，目的是求得各个HMM模型的参数。以音素为基本单位的训练过程如下：
    首先，对每个音素建立一个HMM模型，并对模型参数$\lambda $进行初始化。
    每一轮的训练过程中，根据HMM模型参数，先使用前向后向算法计算（HMM概率问题\ref{section:probability}）在所有时间$t$上的前向得分和后向得分，随后进行Baum-Welch
    算法（HMM学习问题\ref{section:learn}）对模型参数进行更新，其中E步计算观测下各状态出现、各状态转移、由状态$i$转移到状态$j$的期望，并计算混合高斯分量权重的期望，
    M步对状态转移矩阵，各状态概率密度函数混合高斯的均值、协方差矩阵参数进行更新。
    重复进行训练过程，直至满足预设条件终止。

    在已经得到各个HMM的模型参数后，识别过程就是计算给定观测在不同HMM模型上的概率，选取概率最大HMM模型对应的状态序列即为识别结果，也就是使用Viterbi算法（HMM预测问题\ref{section:predict}）。
    在大词汇量连续语音识别中，为了提高解码效率，会采取一定的剪枝策略，如Beam Search等。

%\section{基于HTK的连续语音识别系统}
%    \subsection{HTK工具包介绍}
%
%    HTK(HMM Tools Kit)\cite{young1997htk}是一个剑桥大学开发的专门用于建立和处理HMM的实验工具包,主要应用于语音识别领域,也可以应用于语音合成、字符识别和
%    DNA排序等领域。HTK经过剑桥大学、Entropic公司及Microsoft公司的不断增强和改进,使其在世界范围内得到广泛应用。并且，HTK源代码开放，
%    基于ANSI C实现，所以具有良好可移植性和跨平台性。HTK由库程序和工具程序两部分组成。库程序提供一系列函数和接口；工具程序为语音分析，
%    HMM训练、测试和结果分析提供一系列实现。
%
%    其中本毕设中主要使用HTK核心模块HEREST（训练工具）和HVite（识别工具）。
%
%
%    \subsection{HTK连续语音识别系统搭建}
%    \begin{figure}[h]
%        \centering
%        \includegraphics[width=0.8\textwidth]{figures/chapter2/asr_system}
%        \caption{连续语音识别系统框架}
%        \label{fig:asr_system}
%    \end{figure}
%
%    一个连续语音识别系统架构如图\ref{fig:asr_system}所示，由图可以看出，整个系统由训练和识别两大部分组成，在HTK中其对应核心实现部分为HEREST和HVite。
%
%    本节介绍基于HTK连续语音识别系统搭建的数据准备、模型训练和识别系统。
%
%    数据准备部分主要准备字典、词典和语音数据及其对应标注抄本。本毕设中主要依托实验室的资源。
%
%    模型训练过程为：对语音数据进行特征提取；创建单音素模型；加入上下文信息创建三音素模型，三音素模型状态绑定，最后对HMM模型逐步增加混合高斯数。
%    训练过程中重复使用到HEREST。
%
%    识别系统首先定义任务所需的词网络，根据训练过程中产生的模型文件，先对输入语音进行特征提取，然后由识别器词网络在上根据声学模型和语言模型进行打分，进而
%    得到识别结果。识别过程核心为HVite，为了移植HVite到DSP，本毕设对HVite源代码进行了解析。

